{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup library imports\n",
    "import io, time, json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Basic HTTP Requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    return response.status_code, response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read yelp API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(filepath=\"yelp_api_key.txt\"):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "    \n",
    "    # Feel free to modify this function if you are storing the API Key differently\n",
    "    return Path(filepath).read_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get yelp business Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "### TESTING yelp_search: PASSED 2/2\n###\n\n"
    }
   ],
   "source": [
    "def yelp_search_test(yelp_search):\n",
    "    total, business = yelp_search(read_api_key(), \"Pittsburgh\")\n",
    "    test.true(abs(total - 2600) < 60)\n",
    "    expected_keys = ['id', 'name', 'phone', 'review_count']\n",
    "    if len(business):\n",
    "        test.true(all(k in set(business[0].keys()) for k in expected_keys))\n",
    "\n",
    "@test\n",
    "def yelp_search(api_key, query):\n",
    "    \"\"\"\n",
    "    Make an authenticated request to the Yelp API.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        total (integer): total number of businesses on Yelp corresponding to the query\n",
    "        businesses (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "    headers = {\"Authorization\" : \"Bearer %s\" % (api_key)}\n",
    "    params = {\"location\" : query}\n",
    "    response = requests.get(url, params = params, headers = headers)\n",
    "    result = json.loads(response.text)\n",
    "\n",
    "    return result[\"total\"], result[\"businesses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "### TESTING all_restaurants: PASSED 0/0\n# Cannot locate test function named all_restaurants_test\n###\n\n"
    }
   ],
   "source": [
    "@test\n",
    "def all_restaurants(api_key, query):\n",
    "    \"\"\"\n",
    "    Retrieve ALL the restaurants on Yelp for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "    headers = {\"Authorization\" : \"Bearer %s\" % (api_key)}\n",
    "    offset = 0\n",
    "    params = {\"location\" : query, \"categories\": \"restaurants\", \"limit\": 40}\n",
    "    response = requests.get(url, params = params, headers = headers)\n",
    "    result = json.loads(response.text)\n",
    "    total = result[\"total\"]\n",
    "    final = []\n",
    "    \n",
    "    while offset<total:\n",
    "        params[\"offset\"] = offset\n",
    "        response = requests.get(url, params = params, headers = headers)\n",
    "        offset += 40\n",
    "        result = json.loads(response.text)\n",
    "        final += result[\"businesses\"]\n",
    "        time.sleep(0.2)\n",
    "    return final\n",
    "\n",
    "def all_restaurants_test(all_restaurants):\n",
    "    res = ['Morcilla', 'Umami', 'Piccolo Forno']\n",
    "    data = all_restaurants(read_api_key(), 'Polish Hill, Pittsburgh')\n",
    "    names = [x['name'] for x in data]\n",
    "    \n",
    "    for ele in res:\n",
    "        test.true(ele in names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2.5: Parse the API Responses and Extract the URLs\n",
    "\n",
    "Because we want to seperate the __downloading__ from the __parsing__, fill in the following function to parse the URLs pointing to the restaurants on `yelp.com`. As input your function should expect a string of [properly formatted JSON](http://www.json.org/) (which is similar to __BUT__ not the same as a Python dictionary) and as output should return a Python list of strings. The input JSON will be structured as follows (same as the [sample](https://www.yelp.com/developers/documentation/v3/business_search) on the Yelp API page):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_src = \"\"\"{\n",
    "  \"total\": 8228,\n",
    "  \"businesses\": [\n",
    "    {\n",
    "      \"rating\": 4,\n",
    "      \"price\": \"$\",\n",
    "      \"phone\": \"+14152520800\",\n",
    "      \"id\": \"four-barrel-coffee-san-francisco\",\n",
    "      \"is_closed\": false,\n",
    "      \"categories\": [\n",
    "        {\n",
    "          \"alias\": \"coffee\",\n",
    "          \"title\": \"Coffee & Tea\"\n",
    "        }\n",
    "      ],\n",
    "      \"review_count\": 1738,\n",
    "      \"name\": \"Four Barrel Coffee\",\n",
    "      \"url\": \"https://www.yelp.com/biz/four-barrel-coffee-san-francisco\",\n",
    "      \"coordinates\": {\n",
    "        \"latitude\": 37.7670169511878,\n",
    "        \"longitude\": -122.42184275\n",
    "      },\n",
    "      \"image_url\": \"http://s3-media2.fl.yelpcdn.com/bphoto/MmgtASP3l_t4tPCL1iAsCg/o.jpg\",\n",
    "      \"location\": {\n",
    "        \"city\": \"San Francisco\",\n",
    "        \"country\": \"US\",\n",
    "        \"address2\": \"\",\n",
    "        \"address3\": \"\",\n",
    "        \"state\": \"CA\",\n",
    "        \"address1\": \"375 Valencia St\",\n",
    "        \"zip_code\": \"94103\"\n",
    "      },\n",
    "      \"distance\": 1604.23,\n",
    "      \"transactions\": [\"pickup\", \"delivery\"]\n",
    "    }\n",
    "  ],\n",
    "  \"region\": {\n",
    "    \"center\": {\n",
    "      \"latitude\": 37.767413217936834,\n",
    "      \"longitude\": -122.42820739746094\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "### TESTING parse_api_response: PASSED 1/1\n###\n\n"
    }
   ],
   "source": [
    "def parse_api_response_test(parse_api_response):\n",
    "    test.equal(parse_api_response(json_src), ['https://www.yelp.com/biz/four-barrel-coffee-san-francisco'])\n",
    "\n",
    "@test\n",
    "def parse_api_response(data):\n",
    "    \"\"\"\n",
    "    Parse Yelp API results to extract restaurant URLs.\n",
    "    \n",
    "    Args:\n",
    "        data (string): String of properly formatted JSON.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of URLs as strings from the input JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [ele[\"url\"] for ele in json.loads(data)[\"businesses\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As we can see, JSON is quite trivial to parse (which is not the case with HTML as we will see in a second) and work with programmatically. This is why it is one of the most ubiquitous data serialization formats (especially for RESTful APIs) and a huge benefit of working with a well defined API if one exists. But APIs do not always exists or provide the data we might need, and as a last resort we can always scrape web pages..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Web Pages (and HTML)\n",
    "\n",
    "Think of APIs as similar to accessing a application's database itself (something you can interactively query and receive structured data back). But the results are usually in a somewhat raw form with no formatting or visual representation (like the results from a database query). This is a benefit _AND_ a drawback depending on the end use case. For data science and _programatic_ analysis this raw form is quite ideal, but for an end user requesting information from a _graphical interface_ (like a web browser) this is very far from ideal since it takes some cognitive overhead to interpret the raw information. And vice versa, if we have HTML it is quite easy for a human to visually interpret it, but to try to perform some type of programmatic analysis we first need to parse the HTML into a more structured form.\n",
    "\n",
    "As a general rule of thumb, if the data you need can be accessed or retrieved in a structured form (either from a bulk download or API) prefer that first. But if the data you want (and need) is not as in our case we need to resort to alternative (messier) means.\n",
    "\n",
    "Going back to the \"hello world\" example of question 1 with the NYT, we will do something similar to retrieve the HTML of the Yelp site itself (rather than going through the API) programmatically as text. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Parse a Yelp restaurant Page\n",
    "\n",
    "Using `BeautifulSoup`, parse the HTML of a single Yelp restaurant page to extract the reviews in a structured form as well as the total number of pages. Fill in following function stubs to parse a single page of reviews and return:\n",
    "* the reviews as a structured Python dictionary\n",
    "* the total number of pages of reviews.\n",
    "\n",
    "For each review be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'author': 'Aaron W.' # str\n",
    "    'rating': 4.0        # float\n",
    "    'date': '2019-01-03' # str, yyyy-mm-dd\n",
    "    'description': \"Wonderful!\" # str\n",
    "}\n",
    "```\n",
    "\n",
    "Return reviews in the order that they are present on the page.\n",
    "\n",
    "There can be issues with Beautiful Soup using various parsers, for maximum conpatibility (and fewest errors) initialize the library with the default (and Python standard library parser): `BeautifulSoup(markup, \"html.parser\")`. You may notice that the HTML is automatically generated. Yelp uses a modern web application technology called [React](https://reactjs.org/), which generates the markup from Javascript code. This is a common hazard of scraping data from HTML.\n",
    "\n",
    "Here's a hint for this problem: [`aria`](https://www.w3.org/WAI/PF/aria-1.1/terms) is a web standard that supports text-to-speech, and `aria-*` properties are often used to tag elements that have some semantic meaning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "### TESTING parse_page: PASSED 5/6\n# 5\t: Failed: 33 is not equal to 32\n###\n\n"
    }
   ],
   "source": [
    "# You do not need to use regular expressions in this solution. This is only for testing.\n",
    "import re\n",
    "\n",
    "def reviews_check(reviews):\n",
    "    type_check = lambda field, typ: all(field in r and typ(r[field]) for r in reviews)\n",
    "    test.true(type_check(\"author\", lambda r: isinstance(r, str)))\n",
    "    test.true(type_check(\"rating\", lambda r: isinstance(r, float)))\n",
    "\n",
    "    datecheck = re.compile(\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    test.true(type_check(\"date\", lambda r: datecheck.match(r)))\n",
    "    test.true(type_check(\"description\", lambda r: isinstance(r, str)))\n",
    "\n",
    "def parse_page_test(parse_page):\n",
    "    reviews, num_pages = parse_page(retrieve_html(\"https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\")[1])\n",
    "    reviews_check(reviews)\n",
    "    test.equal(len(reviews), 20)\n",
    "    test.equal(num_pages, 32)\n",
    "    \n",
    "import math\n",
    "@test\n",
    "def parse_page(html):\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    result = []\n",
    "    reviews = soup.find_all(\"script\",attrs={\"type\":\"application/ld+json\"})[0]\n",
    "    reviews = json.loads(reviews.contents[0])\n",
    "    reviews_count = reviews[\"aggregateRating\"][\"reviewCount\"]\n",
    "    reviews = reviews[\"review\"]\n",
    "    for review in reviews:\n",
    "        tmp = {}\n",
    "        tmp[\"author\"] = review[\"author\"]\n",
    "        tmp[\"rating\"] = float(review[\"reviewRating\"][\"ratingValue\"])\n",
    "        tmp[\"date\"] = review[\"datePublished\"]\n",
    "        tmp[\"description\"] = review[\"description\"]\n",
    "        result.append(tmp)\n",
    "    \n",
    "    links = soup.find_all(\"link\",attrs={\"rel\":\"next\"},href=True)\n",
    "    next_link = links[0][\"href\"] if links else None\n",
    "    return result, math.ceil(reviews_count/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q 3.5: Extract all of the Yelp reviews for a Single Restaurant\n",
    "\n",
    "So now that we have parsed a single page, and figured out a method to go from one page to the next we are ready to combine these two techniques and actually crawl through web pages! \n",
    "\n",
    "Using `requests`, programmatically retrieve __ALL__ of the reviews for a __single__ restaurant (provided as a parameter). Just like the API was paginated, the HTML paginates its reviews (it would be a very long web page to show 300 reviews on a single page) and to get all the reviews you will need to parse and traverse the HTML. As input your function will receive a URL corresponding to a Yelp restaurant. As output return a list of dictionaries (structured the same as question 3 containing the relevant information from the reviews.\n",
    "\n",
    "Return reviews in the order that they are present on the page.\n",
    "\n",
    "You will need to get the number of pages on the first request and generate the URL for subsequent pages automatically. Use the Yelp website to see how the URL changes for subsequent pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "### TESTING extract_reviews: PASSED 4/5\n# 0\t: Failed: 46 is not equal to 47\n###\n\n"
    }
   ],
   "source": [
    "def extract_reviews_test(extract_reviews):\n",
    "    reviews = extract_reviews(\"https://www.yelp.com/biz/larry-and-carols-pizza-pittsburgh\")\n",
    "    test.equal(len(reviews), 47) # This may change!\n",
    "    reviews_check(reviews)\n",
    "\n",
    "@test\n",
    "def extract_reviews(url):\n",
    "    \"\"\"\n",
    "    Retrieve ALL of the reviews for a single restaurant on Yelp.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): Yelp URL corresponding to the restaurant of interest.\n",
    "\n",
    "    Returns:\n",
    "        reviews (list): list of dictionaries containing extracted review information\n",
    "    \"\"\"\n",
    "    tmp, pages = parse_page(retrieve_html(url)[1])\n",
    "    res = []\n",
    "    for i in range(pages):\n",
    "        if i>0:\n",
    "            current, tmp_count = parse_page(retrieve_html(url+\"?start=\"+str(20*i))[1])\n",
    "        else:\n",
    "            current = tmp\n",
    "        res+=current\n",
    "    \n",
    "    return res\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}