{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_hw7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 15618\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = \"spacy\")\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m5igjmSXuZg5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_file = \"Pittsburgh_review.train\"\n",
        "test_file = \"Pittsburgh_review.test\"\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = [[] for _ in range(4)]\n",
        "\n",
        "print(pd.read_csv(train_file))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "colab_type": "code",
        "id": "GjHeLZKQmel1",
        "outputId": "1bab040e-db69-4db3-84c8-556f5de07062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\t Len:\t 17500 {'text': ['Undoubtedly', 'the', 'best', 'heavy', 'metal', 'horror', 'item', 'made', 'in', 'the', 'manically', 'headbangin', \"'\", '80', \"'s\", ',', 'which', 'admittedly', 'does', \"n't\", 'sound', 'like', 'much', 'considering', 'how', 'utterly', 'abysmal', 'many', 'other', 'entries', 'in', 'this', 'odd', 'little', 'fright', 'film', 'sub', '-', 'genre', 'like', '\"', 'Hard', 'Rock', 'Zombies', ',', '\"', '\"', 'Blood', 'Tracks', ',', '\"', '\"', 'Terror', 'on', 'Tour', ',', '\"', 'and', 'the', 'especially', 'ungodly', 'Jon', '-', 'Mikl', 'Thor', '-', 'starring', 'stinker', '\"', \"Rock'n'Roll\", 'Nightmare', '\"', 'tended', 'to', 'be', '.', 'That', 'aside', ',', 'this', 'one', 'still', 'deserves', 'props', 'for', 'downplaying', 'the', 'excessive', 'splatter', 'and', 'needlessly', 'flashy', 'special', 'f', '/', 'x', 'razzle', '-', 'dazzle', 'in', 'favor', 'of', 'focusing', 'on', 'adolescent', 'high', 'school', 'characters', 'who', 'are', 'depicted', 'with', 'greater', 'acuity', 'and', 'plausibility', 'than', 'the', 'norm', 'for', 'a', 'mid-80', \"'s\", 'teen', '-', 'targeted', 'scarefest', '.', 'Moreover', ',', 'the', 'film', \"'s\", 'pointed', 'sardonic', 'parodying', 'of', 'both', 'ridiculously', 'overblown', '80', \"'s\", 'heavy', 'metal', 'stupidity', 'and', 'the', 'nauseating', 'self', '-', 'righteousness', 'of', 'the', 'uptight', 'killjoy', 'conservative', 'stiffs', 'who', 'claimed', 'it', 'was', 'the', 'devil', \"'s\", 'music', 'are', 'very', 'clever', 'and', 'on', 'the', 'money', 'funny', '(', 'famed', 'Greed', 'Decade', 'heavy', 'metal', 'god', 'Ozzy', 'Osbourne', 'has', 'a', 'hilarious', 'bit', 'as', 'a', 'smarmy', 'anti', '-', 'metal', 'TV', 'evangelist!).<br', '/><br', '/>Marc', 'Price', '(', 'the', 'hopelessly', 'dweeby', 'Skippy', 'on', '\"', 'Family', 'Ties', '\"', ')', 'gives', 'a', 'surprisingly', 'strong', 'and', 'winning', 'performance', 'as', 'Eddie', '\"', 'Ragman', '\"', 'Weinbauer', ',', 'a', 'geeky', ',', 'socially', 'awkward', 'and', 'severely', 'persecuted', 'heavy', 'metal', 'aficionado', 'who', \"'s\", 'constantly', 'picked', 'on', 'by', 'the', 'stuck', '-', 'up', 'jerk', 'preppie', 'bullies', 'who', 'make', 'up', 'the', 'majority', 'of', 'the', 'student', 'body', 'at', 'Lakeridge', 'High', 'School', '(', 'the', 'cruelty', 'and', 'mean', '-', 'spiritedness', 'of', 'the', 'high', 'school', 'kids', 'is', 'nailed', 'with', 'painfully', 'credible', 'accuracy', ')', '.', 'Eddie', \"'s\", 'life', 'takes', 'a', 'turn', 'for', 'the', 'worse', 'when', 'his', 'rock', 'star', 'idol', 'Sammi', 'Curr', '(', 'an', 'impressively', 'whacked', '-', 'out', 'portrayal', 'by', 'Tony', 'Fields', ')', 'perishes', 'in', 'a', 'hotel', 'fire', '.', 'Hip', 'local', 'disc', 'jockey', 'Nuke', '(', 'KISS', 'front', '-', 'man', 'Gene', 'Simmons', 'in', 'a', 'cool', 'cameo', ')', 'hooks', 'Eddie', 'up', 'with', 'Sammi', \"'s\", 'final', ',', 'unreleased', 'album', ',', 'which', 'when', 'played', 'backwards', 'resurrects', 'Curr', \"'s\", 'malevolent', 'spirit', 'back', 'from', 'the', 'dead', '.', 'Sammi', 'encourages', 'Eddie', 'to', 'sic', 'him', 'on', 'all', 'the', 'vile', 'scumbags', 'who', 'make', 'poor', 'Eddie', \"'s\", 'life', 'the', 'proverbial', 'living', 'hell', ',', 'only', 'to', 'have', 'meek', 'Eddie', 'prove', 'to', 'be', 'a', 'most', 'reluctant', 'would', '-', 'be', 'accomplice', '.', 'It', \"'s\", 'up', 'to', 'Eddie', ',', 'assisted', 'by', 'token', 'nice', 'girl', 'Leslie', 'Graham', '(', 'likeably', 'essayed', 'by', 'the', 'lovely', 'Lisa', 'Orgolini', ')', ',', 'to', 'stop', 'Sammi', 'before', 'things', 'get', 'too', 'out', 'of', 'hand.<br', '/><br', '/>Ably', 'directed', 'with', 'commendable', 'thoughtfulness', 'and', 'sensitivity', 'by', 'character', 'actor', 'Charles', 'Martin', 'Smith', '(', 'who', 'also', 'briefly', 'appears', 'as', 'a', 'nerdy', 'school', 'teacher', ')', ',', 'smartly', 'written', 'by', 'Michael', 'S.', 'Murphy', ',', 'Joel', 'Soisson', ',', 'and', 'Rhet', 'Topham', ',', 'and', 'capably', 'acted', 'by', 'a', 'uniformly', 'up', '-', 'to', '-', 'snuff', 'cast', ',', 'this', 'surefire', 'sleeper', 'even', 'comes', 'complete', 'with', 'a', 'handful', 'of', 'nifty', '\"', 'jump', '\"', 'moments', '(', 'an', 'outrageous', 'attack', 'in', 'the', 'back', 'of', 'a', 'car', 'by', 'a', 'grotesquely', 'lecherous', 'long', '-', 'tongued', 'mutant', 'thingie', 'rates', 'as', 'the', 'definite', 'highlight', ')', ',', 'a', 'rousing', '\"', 'Carrie', '\"-', 'style', 'high', 'school', 'dance', 'slaughter', 'sequence', ',', 'a', 'neatly', 'utilized', 'Halloween', 'setting', ',', 'revenge', 'being', 'correctly', 'shown', 'as', 'a', 'truly', 'ugly', 'business', ',', 'and', 'a', 'solid', 'central', 'message', 'that', 'you', 'should', \"n't\", 'make', 'a', 'particular', 'over', '-', 'hyped', 'person', 'your', 'hero', 'strictly', 'because', 'of', 'the', 'calculated', 'anti', '-', 'establishment', 'posturing', 'said', 'fellow', 'does', 'to', 'qualify', 'for', 'that', 'special', 'status', '.'], 'label': 'pos'}\n",
            "Val\t Len:\t 7500 {'text': ['Even', 'if', 'it', 'were', 'remotely', 'funny', ',', 'this', 'mouldy', 'waxwork', 'of', 'a', 'film', 'would', 'still', 'be', 'soberingly', 'disrespectful', '.', 'Stopping', 'just', 'short', 'of', 'digging', 'up', 'the', 'boys', \"'\", 'corpses', 'and', 're', '-', 'enacting', \"'\", 'Weekend', 'At', 'Bernie', \"'s\", \"'\", '\\x96', 'but', 'only', 'just', '\\x96', 'producer', 'Larry', 'Harmon', 'and', 'the', 'director', 'of', 'the', 'frickin', \"'\", \"'\", 'Ernest', \"'\", 'films', 'use', 'holding', 'the', 'copyright', 'as', 'an', 'excuse', 'to', 'crap', 'all', 'over', 'Stan', 'and', 'Ollie', \"'s\", 'legacy', '.', 'Gailard', 'Sartain', 'does', 'a', 'fair', 'Ollie', 'impersonation', 'but', 'Bronson', 'Pinchot', 'would', \"n't\", 'reach', 'tenth', 'place', 'in', 'a', 'Stan', 'lookalike', 'contest', ';', 'even', 'if', 'they', 'were', 'both', 'spot', 'on', 'the', 'film', 'would', 'be', 'no', 'less', 'detestable', '.', 'The', 'less', 'said', 'about', 'the', 'surrounding', 'catastrophe', 'the', 'better', '.', 'Makes', \"'\", 'Utopia', \"'\", 'look', 'like', 'a', 'dignified', 'swan', 'song', '.'], 'label': 'neg'}\n",
            "Test\t Len:\t 25000 {'text': ['Even', 'the', 'Maria', 'Montez', '/', 'Jon', 'Hall', 'technicolored', 'baubles', 'of', 'the', \"'\", '40s', 'are', 'eclipsed', 'by', '\"', 'Princess', 'of', 'the', 'Nile', ',', '\"', 'Fox', \"'s\", 'entry', 'in', 'Hollywood', \"'s\", \"mid-'50s\", 'obsession', 'with', 'things', 'Egyptian', '(', 'see', '\"', 'Land', 'of', 'the', 'Pharoahs', ',', '\"', '\"', 'Valley', 'of', 'the', 'Kings', ',', '\"', 'etc', '.', ')', 'Pure', ',', 'unadulterated', ',', 'mindless', 'hokum', ',', 'lavishly', 'produced', '(', 'low', '-', 'budgeted', ',', 'actually', ',', 'but', 'using', 'sets', 'and', 'costumes', 'left', 'over', 'from', '\"', 'The', 'Robe', ',', '\"', 'this', 'Technicolored', 'spectacle', 'looks', 'like', 'it', 'cost', 'millions', ')', '.', '71', 'minutes', 'of', 'eye', '-', 'candy', '(', 'the', 'plot', ',', 'having', 'something', 'to', 'do', 'with', 'nefarious', 'derrings', '-', 'do', 'in', 'ancient', 'Egypt', ',', 'is', 'beside', 'the', 'point', ')', 'offers', 'the', 'cinematographer', 'and', 'audiences', 'the', 'delectable', 'sight', 'of', 'Debra', 'Paget', 'wearing', 'an', 'assortment', 'of', 'see', '-', 'thru', 'veils', ',', 'most', 'of', 'which', 'hit', 'the', 'ground', 'when', 'she', 'shakes', 'and', 'shimmies', 'thru', 'a', 'slave', '-', 'girl', 'production', 'number', 'unparalleled', 'in', 'film', 'history', '.', 'Female', 'moviegoers', 'were', 'not', 'shortchanged', ':', 'Fox', \"'s\", 'handsomest', 'young', 'contract', 'player', ',', 'Jeffrey', 'Hunter', ',', 'is', 'as', 'photogenic', 'as', 'Ms.', 'Paget', ',', 'while', 'Michael', 'Rennie', 'lurks', 'around', 'in', 'the', 'background', ',', 'stirring', 'up', 'evil', 'doings', 'in', 'the', 'land', 'of', 'the', 'pyramids', '.', 'For', 'those', 'who', 'might', 'think', 'Paget', '&', 'Hunter', 'ca', \"n't\", 'act', 'and', 'were', 'only', 'hired', 'for', 'their', 'physical', 'attributes', ',', 'check', 'out', 'their', 'subtle', ',', 'overlooked', ',', 'heartbreaking', 'work', 'together', 'a', 'few', 'years', 'later', 'in', '\"', 'White', 'Feather', '\"', '(', 'another', 'Fox', 'production', 'that', 'has', 'sadly', 'vanished', 'into', 'the', 'realm', 'of', '\"', 'lost', 'films', '\"', ')', '.', '\"', 'Princess', 'of', 'the', 'Nile', '\"', 'still', 'stands', 'in', 'a', 'class', 'by', 'itself', 'as', 'a', 'cheerfully', 'mindless', ',', 'breathlessly', 'fast', '-', 'paced', ',', 'dazzling', 'testament', 'to', 'the', 'glories', 'of', '3-strip', 'Technicolor', '--', 'and', 'the', 'seductive', 'charms', 'of', 'Ms.', 'Paget', '(', 'all', 'of', '20', 'at', 'the', 'time', ')', '.', 'Put', 'this', 'one', '-', 'of', '-', 'a', '-', 'kind', 'kitsch', 'classic', 'at', 'the', 'top', 'of', 'your', '\"', 'guilty', 'pleasures', '\"', 'list', ',', 'and', 'enjoy', '.', 'Satisfaction', 'guaranteed', '!'], 'label': 'pos'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "DEV_CSV = \"dev_data.csv\"\n",
        "\n",
        "f_train_data = \"dev_text.txt\"\n",
        "f_train_label = \"dev_label.txt\"\n",
        "f_test_data = \"heldout_text.txt\"\n",
        "\n",
        "dev_data , dev_labels , test_data = [] , [] ,[]\n",
        "\n",
        "with open(f_train_data,\"r\",encoding = \"utf-8\") as fd1 , open(f_train_label,\"r\",encoding = \"utf-8\") as fd2 , open(f_test_data,\"r\",encoding = \"utf-8\") as fd3:\n",
        "    for line in fd1:\n",
        "        dev_data.append(line.strip().replace(\"<br /><br />\",\" \"))\n",
        "    for line in fd2:\n",
        "        dev_labels.append(line.strip())\n",
        "    for line in fd3:\n",
        "        # test_data.append([token.text for token in nlp.tokenizer(line.strip().replace(\"<br /><br />\",\" \"))])\n",
        "        test_data.append(line.strip().replace(\"<br /><br />\",\" \"))\n",
        "\n",
        "DEV_DATA = {\"text\":dev_data,\"label\":dev_labels}\n",
        "TEST_DATA = {\"text\":test_data}\n",
        "\n",
        "df_dev = DataFrame(DEV_DATA,columns = [\"text\",\"label\"])\n",
        "df_dev.to_csv(DEV_CSV)\n",
        "\n",
        "df_test = DataFrame(TEST_DATA,columns = [\"text\"])\n",
        "df_test.to_csv(TEST_CSV)\n",
        "\n",
        "dev_set = data.TabularDataset(path = DEV_CSV,format = \"csv\",fields = [(\"id\",None),(\"text\",TEXT),(\"label\",LABEL)],skip_header = True)\n",
        "test_data = data.TabularDataset(path = TEST_CSV,format = \"csv\" , fields = [(\"id\",None),(\"text\",TEXT)],skip_header= True)\n",
        "\n",
        "train_data , valid_data = dev_set.split()\n",
        "\n",
        "# # full data set\n",
        "from torchtext import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "print(\"Train\\t\",\"Len:\\t\",len(train_data),vars(train_data.examples[0]))\n",
        "print(\"Val\\t\"  ,\"Len:\\t\",len(valid_data),vars(valid_data.examples[0]))\n",
        "print(\"Test\\t\" ,\"Len:\\t\",len(test_data) ,vars(test_data.examples[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "colab_type": "code",
        "id": "VmuSB6R8nPSq",
        "outputId": "950c2350-e3f2-43c9-eb1d-b68f2d9798a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT: 20002 LABEL: 2\n",
            "[('the', 202946), (',', 191897), ('.', 165203), ('and', 109353), ('a', 109090), ('of', 100593), ('to', 93492), ('is', 76388), ('in', 61239), ('I', 53898), ('it', 53471), ('that', 49216), ('\"', 44495), (\"'s\", 43114), ('this', 42033), ('-', 36763), ('/><br', 35671), ('was', 34954), ('as', 30235), ('with', 29932)]\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            "defaultdict(<function _default_unk_index at 0x7ff2e59af400>, {'pos': 0, 'neg': 1})\n"
          ]
        }
      ],
      "source": [
        "MAX_VOCAB_SIZE = 20_000\n",
        "\n",
        "TEXT.build_vocab(train_data,max_size=MAX_VOCAB_SIZE,vectors=\"glove.6B.200d\", unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(\"TEXT:\",len(TEXT.vocab),\"LABEL:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aYVew_XHpO0k"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "        (train_data, valid_data),\n",
        "        sort_within_batch=True, \n",
        "        sort_key=lambda x: len(x.text),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6T13KNJWpRHj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "DROP = 0.5\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):       \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)  \n",
        "        self.rnn = nn.LSTM(embedding_dim,hidden_dim,2,dropout = 0,bidirectional = True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(DROP)\n",
        "        \n",
        "    def forward(self, text):\n",
        "      \n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "#         res = self.dropout(torch.sum(hidden[0],0))\n",
        "#         print(output.size())\n",
        "        res = self.dropout(torch.mean(output,0))\n",
        "        return self.fc(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qdWCQ0pipdVS"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "5dqfiasqpk2F",
        "outputId": "9a458ede-1ee5-491f-f4ed-0284ead673ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 2,631,241 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tQh7BoTVppRr"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "97XeoviNpuwT"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vTGrzYjEpxWo"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6sxKG0IFqjsJ"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e7MXUfVWqn8e"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator: \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2PXE3BWEqwng"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wk65fYi_q3Ry"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "colab_type": "code",
        "id": "r_LRPaWQq7Em",
        "outputId": "2db081fb-791d-4525-8dc9-674dce858972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.643 | Train Acc: 62.89%\n",
            "\t Val. Loss: 0.593 |  Val. Acc: 67.69%\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.486 | Train Acc: 76.90%\n",
            "\t Val. Loss: 0.431 |  Val. Acc: 81.26%\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.356 | Train Acc: 84.89%\n",
            "\t Val. Loss: 0.369 |  Val. Acc: 84.46%\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.276 | Train Acc: 88.93%\n",
            "\t Val. Loss: 0.378 |  Val. Acc: 84.63%\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.222 | Train Acc: 91.43%\n",
            "\t Val. Loss: 0.347 |  Val. Acc: 86.88%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        \n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ]
    }
  ]
}