{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_hw7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 15618\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = \"spacy\")\n",
        "LABEL = data.LabelField(dtype = torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m5igjmSXuZg5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[\"It's almost amazing that this place gets busy.  I had a breakfast burrito that came with home fries. Hands down most bland, sub par breakfast burrito I've ever come across.  Eggs were over cooked and without seasoning.  I opted for the up charge of chorizo, there was maybe 1 oz of meat in it.  Not great by any measure. There was tater tots in there also but super soggy, as though they were fried in lukewarm oil.  The homefries that came with it were of the same family.  I only have deep regret I wasted a meal here.\"] [1.0]\n[\"Honestly, I was going to give Tana 4 stars, but considering it's the only surviving Ethiopian restaurant in Pittsburgh that I can find I figured screw it! They deserve all 5. My boyfriend and I went here on a weeknight, and there was one other couple there. Throughout our meal 2 or three other couples came and went. So it's definitely quiet and empty which can be a little awkward for some, but I loved it & the waiter didn't make it weird. We got sambusa to start, some Ethiopian beers, and the Tana sampler for two. And OMG we were stuffed. Realistically, the Tana sampler for 1 probably would've been plenty. We had lots leftover and very full bellies. I love the communal style of eating, and would go back again any time. But this definitely isn't the place for you if you don't like to touch your food, or if you're afraid of sharing!\"] [5.0]\n"
        }
      ],
      "source": [
        "train_file = \"Pittsburgh_review.train\"\n",
        "test_file = \"Pittsburgh_review.test\"\n",
        "\n",
        "train_data, train_labels = pd.read_csv(train_file)[\"text\"].tolist(), pd.read_csv(train_file)[\"rating\"].tolist()\n",
        "print(train_data[:1], train_labels[:1])\n",
        "\n",
        "test_data, test_labels = pd.read_csv(test_file)[\"text\"].tolist(), pd.read_csv(test_file)[\"rating\"].tolist()\n",
        "print(test_data[:1], test_labels[:1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Train\t Len:\t 21952 {'text': ['It', \"'s\", 'almost', 'amazing', 'that', 'this', 'place', 'gets', 'busy', '.', ' ', 'I', 'had', 'a', 'breakfast', 'burrito', 'that', 'came', 'with', 'home', 'fries', '.', 'Hands', 'down', 'most', 'bland', ',', 'sub', 'par', 'breakfast', 'burrito', 'I', \"'ve\", 'ever', 'come', 'across', '.', ' ', 'Eggs', 'were', 'over', 'cooked', 'and', 'without', 'seasoning', '.', ' ', 'I', 'opted', 'for', 'the', 'up', 'charge', 'of', 'chorizo', ',', 'there', 'was', 'maybe', '1', 'oz', 'of', 'meat', 'in', 'it', '.', ' ', 'Not', 'great', 'by', 'any', 'measure', '.', 'There', 'was', 'tater', 'tots', 'in', 'there', 'also', 'but', 'super', 'soggy', ',', 'as', 'though', 'they', 'were', 'fried', 'in', 'lukewarm', 'oil', '.', ' ', 'The', 'homefries', 'that', 'came', 'with', 'it', 'were', 'of', 'the', 'same', 'family', '.', ' ', 'I', 'only', 'have', 'deep', 'regret', 'I', 'wasted', 'a', 'meal', 'here', '.'], 'label': '1.0'}\nTest\t Len:\t 9408 {'text': ['Honestly', ',', 'I', 'was', 'going', 'to', 'give', 'Tana', '4', 'stars', ',', 'but', 'considering', 'it', \"'s\", 'the', 'only', 'surviving', 'Ethiopian', 'restaurant', 'in', 'Pittsburgh', 'that', 'I', 'can', 'find', 'I', 'figured', 'screw', 'it', '!', 'They', 'deserve', 'all', '5', '.', 'My', 'boyfriend', 'and', 'I', 'went', 'here', 'on', 'a', 'weeknight', ',', 'and', 'there', 'was', 'one', 'other', 'couple', 'there', '.', 'Throughout', 'our', 'meal', '2', 'or', 'three', 'other', 'couples', 'came', 'and', 'went', '.', 'So', 'it', \"'s\", 'definitely', 'quiet', 'and', 'empty', 'which', 'can', 'be', 'a', 'little', 'awkward', 'for', 'some', ',', 'but', 'I', 'loved', 'it', '&', 'the', 'waiter', 'did', \"n't\", 'make', 'it', 'weird', '.', 'We', 'got', 'sambusa', 'to', 'start', ',', 'some', 'Ethiopian', 'beers', ',', 'and', 'the', 'Tana', 'sampler', 'for', 'two', '.', 'And', 'OMG', 'we', 'were', 'stuffed', '.', 'Realistically', ',', 'the', 'Tana', 'sampler', 'for', '1', 'probably', 'would', \"'ve\", 'been', 'plenty', '.', 'We', 'had', 'lots', 'leftover', 'and', 'very', 'full', 'bellies', '.', 'I', 'love', 'the', 'communal', 'style', 'of', 'eating', ',', 'and', 'would', 'go', 'back', 'again', 'any', 'time', '.', 'But', 'this', 'definitely', 'is', \"n't\", 'the', 'place', 'for', 'you', 'if', 'you', 'do', \"n't\", 'like', 'to', 'touch', 'your', 'food', ',', 'or', 'if', 'you', \"'re\", 'afraid', 'of', 'sharing', '!'], 'label': '5.0'}\n"
        }
      ],
      "source": [
        "TRAIN_CSV = \"train_data.csv\"\n",
        "TEST_CSV = \"test_data.csv\"\n",
        "\n",
        "TRAIN_DATA = {\"text\":train_data,\"label\":train_labels}\n",
        "TEST_DATA = {\"text\":test_data,\"label\":test_labels}\n",
        "\n",
        "df_train = DataFrame(TRAIN_DATA,columns = [\"text\",\"label\"])\n",
        "df_train.to_csv(TRAIN_CSV)\n",
        "\n",
        "df_test = DataFrame(TEST_DATA,columns = [\"text\", \"label\"])\n",
        "df_test.to_csv(TEST_CSV)\n",
        "\n",
        "train_dataset = data.TabularDataset(path = TRAIN_CSV,format = \"csv\",fields = [(\"id\",None),(\"text\",TEXT),(\"label\",LABEL)],skip_header = True)\n",
        "test_dataset = data.TabularDataset(path = TEST_CSV,format = \"csv\",fields = [(\"id\",None),(\"text\",TEXT),(\"label\",LABEL)],skip_header = True)\n",
        "\n",
        "print(\"Train\\t\",\"Len:\\t\",len(train_dataset),vars(train_dataset.examples[0]))\n",
        "print(\"Test\\t\" ,\"Len:\\t\",len(test_dataset) ,vars(test_dataset.examples[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "TEXT: 12827 LABEL: 5\n[('.', 186417), ('the', 137834), ('and', 111112), (',', 102669), ('I', 86275), ('a', 85040), ('was', 68677), ('to', 63675), ('of', 46897), ('it', 39001), ('is', 36811), ('for', 36459), ('The', 34550), ('!', 33821), ('with', 30665), ('in', 30167), ('but', 26525), ('that', 24240), (' ', 24127), ('were', 23631)]\n['<unk>', '<pad>', '.', 'the', 'and', ',', 'I', 'a', 'was', 'to']\ndefaultdict(None, {'5.0': 0, '4.0': 1, '3.0': 2, '2.0': 3, '1.0': 4})\n"
        }
      ],
      "source": [
        "MAX_VOCAB_SIZE = 20_000\n",
        "\n",
        "TEXT.build_vocab(train_dataset,max_size=MAX_VOCAB_SIZE,vectors=\"glove.6B.200d\", unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "print(\"TEXT:\",len(TEXT.vocab),\"LABEL:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "        (train_dataset, test_dataset),\n",
        "        sort_within_batch=True, \n",
        "        sort_key=lambda x: len(x.text),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "DROP = 0.1\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):       \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)  \n",
        "        self.rnn = nn.LSTM(embedding_dim,hidden_dim,2,dropout = DROP,bidirectional = True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(DROP)\n",
        "        \n",
        "    def forward(self, text):\n",
        "      \n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "#         res = self.dropout(torch.sum(hidden[0],0))\n",
        "#         print(output.size())\n",
        "        res = self.dropout(torch.mean(output,0))\n",
        "        return self.fc(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 512\n",
        "HIDDEN_DIM = 512\n",
        "OUTPUT_DIM = 5\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "The model has 17,074,693 trainable parameters\n"
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    values, indices = torch.max(preds, 1)\n",
        "    res = indices.cpu().detach().numpy()\n",
        "    correct = (y.cpu().detach().numpy()==res)\n",
        "\n",
        "\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    # rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    # correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = sum(correct) / len(correct)\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator: \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        # _, preds = torch.max(predictions,1)\n",
        "        # print(preds.size())\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            # _, preds = torch.max(predictions,1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'float'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-4cdbdd641c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-74636d5999a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(preds.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-6099243a206f>\u001b[0m in \u001b[0;36mbinary_accuracy\u001b[0;34m(preds, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        \n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}